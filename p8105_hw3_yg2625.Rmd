---
title: "hw3_yg2625"
author: "Yue Gu"
date: "October 13, 2018"
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(echo = TRUE,
                      fig.width = 15,
                      fig.height = 5,
                      fig.asp = 0.6,
                      out.width = "90%")

```



# Problem 1
## Data Cleaning
```{r}
data(brfss_smart2010)
brfss_data = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(.data = ., state = locationabbr, county = locationdesc) %>% 
  filter(.data = ., topic == "Overall Health", response == "Excellent" || response == "Very good" || response == "Good" || response == "Fair" || reponse == "Poor") %>% 
  mutate(.data = ., response = as.factor(response)) %>% 
  arrange(.data = .,  match(response,c("Excellent", "Very good", "Good", "Fair", "Poor")))

```

## Answering questions
```{r}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_obs = n_distinct(county) == 7) %>% 
  filter(n_obs == T)
```
### Hence, by the output, CT, FL, NC were observed at 7 locations in 2002.

### The "spaghetti plot"" shown below
```{r}
brfss_data %>% 
  group_by(year, state) %>% 
  summarize(n_obs = n_distinct(county)) %>%
  ggplot(aes(x = year, y = n_obs, color = state)) + geom_line()
```

### Required table shown below
```{r}
brfss_data %>% 
  filter(.data = ., response == "Excellent", state == "NY", year == "2002"|year == "2006"|year == "2010") %>%
  group_by(year, state) %>% 
  summarize(mean_data = mean(data_value)ï¼Œ
            sd_data = sd(data_value)) %>% 
  knitr::kable(digits = 2)
```

### Required five-panel plot shown below
```{r}
brfss_data %>% 
  group_by(year, state, response) %>% 
  summarize(mean_data = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_data, color = state)) + 
    geom_line() + 
    facet_grid(.~response)
```


# Problem 2

## Description of exploration
```{r}
data("instacart")
insta_data =
  instacart

dim(insta_data)
n_distinct(insta_data$order_id)
n_distinct(insta_data$product_id)
n_distinct(insta_data$user_id)
n_distinct(insta_data$aisle_id)
n_distinct(insta_data$department_id)
```
### Hence, by the output, we know the size of the dataset is 1384617 rows with 15 columns, there is 1384617 observations in the datasets. Every observations represent a product from an order, and there is 131209 unique orders, 39123 unique products, 131209 unique users and 21 departments in the datasets.
### Some important key variables may include order_id, product_id, order_number, aisle_id and product_name, which provide us details of a product in an order from an aisle with the name a product.
### For example, for the first observation in our dataset, we know its product ID is 49302 and it's the first item that was added to cart with the history of being ordered by the past.The product's customer ID is 112106 and it belows to the train. The order sequence of the product is the 4th that was placed on Thursday at 10. 9 days since the last order and it's BUlgarian Yogurt from 20th aisle named yogurt in 16th department called dairy eggs.


## Answering questions

```{r}
n_distinct(insta_data$aisle)
head(arrange(count(insta_data, aisle_id), desc(n)), n = 1)
```
### There are 134 unique aisles and the most items ordered aisle is N0.83 with 150609 products.

### Requied Plot shown below
```{r}
insta_data %>% 
  group_by(aisle_id) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = as.character(aisle_id), y = n, color = as.character(aisle_id))) +
    geom_point(alpha = 0.5) +
    viridis::scale_color_viridis(
    name = "Aisle ID", 
    discrete = TRUE
  ) + 
    theme(legend.position = "bottom")
```

### Table for most popopular items in 3 required aisles
```{r}
insta_data %>% 
  filter(.data = ., aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>% 
  filter(min_rank(desc(n)) == 1) %>% 
  knitr::kable()
```

### Table for the mean hour of the day ath 2 product name required
```{r}
insta_data %>% 
  filter(.data = ., product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean_hr = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hr) %>%
  rename(Sun = "0", Mon = "1", Tues = "2", Wed = "3", Thrs = "4", Fri = "5", Sat = "6", Product = product_name) %>% 
  knitr::kable(digit = 2)
```

# Problem 3
## Data exploration

```{r}
data(ny_noaa)
noaa_data = 
  ny_noaa

dim(ny_noaa)
sum(is.na(ny_noaa$prcp))/nrow(noaa_data)
sum(is.na(ny_noaa$snow))/nrow(noaa_data)
sum(is.na(ny_noaa$snwd))/nrow(noaa_data)
sum(is.na(ny_noaa$tmax))/nrow(noaa_data)
sum(is.na(ny_noaa$tmin))/nrow(noaa_data)

```
### By the output, we know the size of the data is 2595176 rows with 7 columns. There is 2595176 observations in the data containing weather statistics collected by each station.
### Some key variables may include date, prcp, snow, tmax, tmin. Date gives us information about the year, month and day of the weather stats, prcp provides us precipitation in tenths of mm, snow tells us snowfall in mm, tmax tells us maximum temp in tenths of degrees C and tmin tells us minimum temp in tenths of degrees C.
### There is lots of missing value in weather stats, there is 5.6% prcp, 14.7% snowfall, 22.8 snowdepth, 43.7% tmax and 43.7% tmin data is missing, which may cause some bias to the statistical analysis output.

## Data cleaning and Answering questions
```{r}
noaa_data_tidy =
  noaa_data %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(.data = ., prcp = as.numeric(prcp)/10, tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10)


head(arrange(count(noaa_data_tidy, snow), desc(n)), n = 1)
```
### 0 is the most commonly obeserved value in snowfall because not every day is snowing. And in the days without snow,there's 0 snowfall recorded and the proportion of days without snow in a year is relatively high.

### Two-panel plot for avg max temp showing average max temp in Jan and July
```{r}
noaa_data_tidy %>% 
  filter(.data = ., month == "01" | month == "07", tmax != "") %>% 
  group_by(id, month, year) %>%
  summarize(tmax_mean = mean(tmax)) %>% 
  ggplot(aes(x = year, y = tmax_mean)) +
    geom_point()+
    facet_grid(. ~ month)
```
### By the graph, we could obverse that January has lower temperature compared to July throughout all year because January is in winter while July is in Summer in NY, which makes the temperature difference. There is several outliers in the graph in years including 1982, 1993, 1999, 2004 and etc. There are some relatively extreme outliers in 2005 Jan and 1988 July.

### Two-panel plot for showing (1) tmax vs tmin (2) distribution of snowfall values by year
```{r}
library(patchwork)
t_vs_p = ggplot(noaa_data_tidy, aes(x = tmax, y = tmin)) +
  geom_hex()

noaa_data_snow =
  noaa_data_tidy %>% 
  filter(.data = ., snow > 0 & snow < 100)

snow_dist_p = ggplot(noaa_data_snow, aes(x = snow, fill = year)) +
  geom_density()

t_vs_p + snow_dist_p

```









